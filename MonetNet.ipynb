{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#import graph, coarsening, utils\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport graph\n",
    "%aimport coarsening\n",
    "%aimport utils\n",
    "\n",
    "import tensorflow as tf\n",
    "import time, shutil\n",
    "import numpy as np\n",
    "import os, collections, sklearn\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import scipy.sparse\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of some flags useful later in the code\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "# Graphs.\n",
    "flags.DEFINE_integer('number_edges', 8, 'Graph: minimum number of edges per vertex.')\n",
    "flags.DEFINE_string('metric', 'euclidean', 'Graph: similarity measure (between features).')\n",
    "flags.DEFINE_bool('normalized_laplacian', True, 'Graph Laplacian: normalized.')\n",
    "flags.DEFINE_integer('coarsening_levels', 4, 'Number of coarsened graphs.')\n",
    "\n",
    "# Directories.\n",
    "flags.DEFINE_string('dir_data', 'data_mnist', 'Directory to store data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_U(dist2,idx,nodes_coordinates):\n",
    "    dist2 = get_dist(nodes_coordinates,idx)\n",
    "    U_x = dist2[:,0]\n",
    "    U_y = dist2[:,1]\n",
    "    M = idx.shape[0]\n",
    "    I = np.arange(0, M).repeat(8)\n",
    "    Ax = sp.coo_matrix((U_x,(I,idx.flatten())),shape=(M,M))\n",
    "    Ay = sp.coo_matrix((U_y,(I,idx.flatten())),shape=(M,M))\n",
    "    A2 = [Ax,Ay]\n",
    "    return A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_U() missing 1 required positional argument: 'nodes_coordinates'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-240-3e56e5a9e2ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes_coordinates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_rows_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0mdist2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_coordinates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0mA2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_U\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mperms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoarsen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoarsening_levels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_U() missing 1 required positional argument: 'nodes_coordinates'"
     ]
    }
   ],
   "source": [
    "#Here we proceed at computing the original grid where the images live and the various coarsening that are applied\n",
    "#for each level\n",
    "\n",
    "def grid_graph(m,raw=False):\n",
    "    z = graph.grid(m)  # normalized nodes coordinates\n",
    "    dist, idx = graph.distance_sklearn_metrics(z, k=FLAGS.number_edges, metric=FLAGS.metric) \n",
    "    #dist contains the distance of the 8 nearest neighbors for each node indicated in z sorted in ascending order\n",
    "    #idx contains the indexes of the 8 nearest for each node sorted in ascending order by distance\n",
    "    A = graph.adjacency(dist, idx) \n",
    "    return A, z,idx,dist\n",
    "\n",
    "def get_dist(frm,to):\n",
    "    dist =[]\n",
    "    for i,from_node in enumerate(frm):\n",
    "       neighbor_idx =  to[i]\n",
    "       neighbors = frm[neighbor_idx]\n",
    "       for n_idx,n in zip(neighbor_idx,neighbors):\n",
    "           diff = from_node-n\n",
    "           dist.append(diff)\n",
    "    #             dist[tuple(from_node.tolist()+[n_idx])] = diff\n",
    "    dist = np.vstack(dist)\n",
    "    return dist\n",
    "\n",
    "def get_U(dist2,idx,nodes_coordinates):\n",
    "    dist2 = get_dist(nodes_coordinates,idx)\n",
    "    U_x = dist2[:,0]\n",
    "    U_y = dist2[:,1]\n",
    "    M = idx.shape[0]\n",
    "    I = np.arange(0, M).repeat(8)\n",
    "    Ax = sp.coo_matrix((U_x,(I,idx.flatten())),shape=(M,M))\n",
    "    Ay = sp.coo_matrix((U_y,(I,idx.flatten())),shape=(M,M))\n",
    "    A2 = [Ax,Ay]\n",
    "    return A2\n",
    "\n",
    "def coarsen(A, levels):\n",
    "    graphs, parents = coarsening.metis(A, levels) #Coarsen a graph multiple times using Graclus variation of the METIS algorithm. \n",
    "                                                  #Basically, we randomly sort the nodes, we iterate on them and we decided to group each node\n",
    "                                                  #with the neighbor having highest w_ij * 1/(\\sum_k w_ik) + w_ij * 1/(\\sum_k w_kj) \n",
    "                                                  #i.e. highest sum of probabilities to randomly walk from i to j and from j to i.\n",
    "                                                  #We thus favour strong connections (i.e. the ones with high weight wrt all the others for both nodes) \n",
    "                                                  #in the choice of the neighbor of each node.\n",
    "                    \n",
    "                                                  #Construction is done a priori, so we have one graph for all the samples!\n",
    "                    \n",
    "                                                  #graphs = list of spare adjacency matrices (it contains in position \n",
    "                                                  #          0 the original graph)\n",
    "                                                  #parents = list of numpy arrays (every array in position i contains \n",
    "                                                  #           the mapping from graph i to graph i+1, i.e. the idx of\n",
    "                                                  #           node i in the coarsed graph -> that is, the idx of its cluster) \n",
    "    perms = coarsening.compute_perm(parents) #Return a list of indices to reorder the adjacency and data matrices so\n",
    "                                             #that two consecutive nodes correspond to neighbors that should be collapsed\n",
    "                                             #to produce the coarsed version of the graph.\n",
    "                                             #Fake nodes are appended for each node which is not grouped with anybody else\n",
    "    laplacians = []\n",
    "    \n",
    "    U = []\n",
    "    for i,A in enumerate(graphs):\n",
    "        print(i)\n",
    "        M, M = A.shape\n",
    "        graph\n",
    "        A, nodes_coordinates,idx,dist = grid_graph(int(M**.5))\n",
    "#         dist2 = get_dist(nodes_coordinates,idx)\n",
    "        A2 = get_U(dist,idx,nodes_coordinates)\n",
    "#         new_A.append(coarsening.perm_adjacency(A_raw,perms[i]))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # We remove self-connections created by metis.\n",
    "        A = A.tocoo()\n",
    "        A.setdiag(0)\n",
    "\n",
    "        if i < levels: #if we have to pool the graph \n",
    "            print(i)\n",
    "            A = coarsening.perm_adjacency(A, perms[i]) #matrix A is here extended with the fakes nodes\n",
    "                                                       #in order to do an efficient pooling operation\n",
    "                                                       #in tensorflow as it was a 1D pooling\n",
    "#             new_A.append(coarsening.perm_adjacency(A_raw,perms[i]))\n",
    "            Ux = coarsening.perm_adjacency(A2[0],perms[i])\n",
    "            Uy = coarsening.perm_adjacency(A2[1],perms[i])\n",
    "            U.append([Ux,Uy])\n",
    "        A = A.tocsr()\n",
    "        A.eliminate_zeros()\n",
    "        Mnew, Mnew = A.shape\n",
    "        print('Layer {0}: M_{0} = |V| = {1} nodes ({2} added), |E| = {3} edges'.format(i, Mnew, Mnew-M, A.nnz//2))\n",
    "\n",
    "        L = graph.laplacian(A, normalized=FLAGS.normalized_laplacian)\n",
    "        laplacians.append(A)\n",
    "\n",
    "    return laplacians, perms[0] if len(perms) > 0 else None, perms, U\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "np.random.seed(0)\n",
    "n_rows_cols = 28\n",
    "A, nodes_coordinates,idx,dist = grid_graph(n_rows_cols,True)\n",
    "dist2 = get_dist(nodes_coordinates,idx)\n",
    "A2 = get_U(dist2,idx)\n",
    "_, perm,perms,U = coarsen(A,FLAGS.coarsening_levels)\n",
    "\n",
    "print('Execution time: {:.2f}s'.format(time.time() - t_start))\n",
    "\n",
    "# graph.plot_spectrum(L)  # plateau is due to all the disconnected fake nodes \n",
    "# !!!! the plateau is in correspondence of 1 and not 0 because we are using a variation of the normalized laplacian which admits a degree different from 0 even in \n",
    "# correspondence of disconnected nodes (normalized laplacian D^-0.5 * L * D^-0.5 = I - D^-0.5 * A * D^-0.5 doesn't exist for D_i==0!).\n",
    "# With normalized_laplacian=False the plateau is indeed in correspondence of 0 (as it should) !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist(frm,to):\n",
    "#     dist = {}\n",
    "   dist =[]\n",
    "   for i,from_node in enumerate(frm):\n",
    "       neighbor_idx =  to[i]\n",
    "       neighbors = frm[neighbor_idx]\n",
    "       for n_idx,n in zip(neighbor_idx,neighbors):\n",
    "           diff = from_node-n\n",
    "           dist.append(diff)\n",
    "#             dist[tuple(from_node.tolist()+[n_idx])] = diff\n",
    "   dist = np.vstack(dist)\n",
    "   return dist\n",
    "\n",
    "def get_U(dist2,idx):\n",
    "    dist2 = get_dist(nodes_coordinates,idx)\n",
    "    U_x = dist2[:,0]\n",
    "    U_y = dist2[:,1]\n",
    "    M = idx.shape[0]\n",
    "    I = np.arange(0, M).repeat(8)\n",
    "    Ax = sp.coo_matrix((U_x,(I,idx.flatten())),shape=(M,M))\n",
    "    Ay = sp.coo_matrix((U_y,(I,idx.flatten())),shape=(M,M))\n",
    "    A2 = [Ax,Ay]\n",
    "    return A2\n",
    "\n",
    "\n",
    "dist2 = get_dist(nodes_coordinates,idx)\n",
    "A2 = get_U(dist2,idx)\n",
    "# U_x = dist2[:,0]\n",
    "# U_y = dist2[:,1]\n",
    "# M = idx.shape[0]\n",
    "# I = np.arange(0, M).repeat(8)\n",
    "# Ax = sp.coo_matrix((U_x,(I,idx.flatten())),shape=(M,M))\n",
    "# Ay = sp.coo_matrix((U_y,(I,idx.flatten())),shape=(M,M))\n",
    "# A2 = [Ax,Ay]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_U(dist2,idx):\n",
    "    dist2 = get_dist(nodes_coordinates,idx)\n",
    "    U_x = dist2[:,0]\n",
    "    U_y = dist2[:,1]\n",
    "    M = idx.shape[0]\n",
    "    I = np.arange(0, M).repeat(8)\n",
    "    Ax = sp.coo_matrix((U_x,(I,idx.flatten())),shape=(M,M))\n",
    "    Ay = sp.coo_matrix((U_y,(I,idx.flatten())),shape=(M,M))\n",
    "    A2 = [Ax,Ay]\n",
    "    return A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dist2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = idx.shape[0]\n",
    "I = np.arange(0, M).repeat(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ax = sp.coo_matrix((U_x,(I,idx.flatten())),shape=(M,M))\n",
    "Ay = sp.coo_matrix((U_y,(I,idx.flatten())),shape=(M,M))\n",
    "A2 = [Ax,Ay]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-8e9e1ea7e7a7>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/heng/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/heng/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /home/heng/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data_mnist/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /home/heng/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data_mnist/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting data_mnist/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data_mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/heng/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Execution time: 1.77s\n"
     ]
    }
   ],
   "source": [
    "#loading of MNIST dataset\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(FLAGS.dir_data, one_hot=False)\n",
    "\n",
    "train_data = mnist.train.images.astype(np.float32)\n",
    "val_data = mnist.validation.images.astype(np.float32) #the first 5K samples of the training dataset \n",
    "                                                      #are used for validation\n",
    "test_data = mnist.test.images.astype(np.float32)\n",
    "train_labels = mnist.train.labels\n",
    "val_labels = mnist.validation.labels\n",
    "test_labels = mnist.test.labels\n",
    "\n",
    "t_start = time.time()\n",
    "train_data = coarsening.perm_data(train_data, perm)\n",
    "val_data = coarsening.perm_data(val_data, perm)\n",
    "test_data = coarsening.perm_data(test_data, perm)\n",
    "print('Execution time: {:.2f}s'.format(time.time() - t_start))\n",
    "del perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChebNet:\n",
    "    \"\"\"\n",
    "    The neural network model.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Helper functions used for constructing the model\n",
    "    def _weight_variable(self, shape,substr, regularization=True): \n",
    "        \"\"\"Initializer for the weights\"\"\"\n",
    "        \n",
    "        initial = tf.truncated_normal_initializer(0, 0.1)\n",
    "        var = tf.get_variable('weights'+substr, shape, tf.float32, initializer=initial)\n",
    "        if regularization: #append the loss of the current variable to the regularization term \n",
    "            self.regularizers.append(tf.nn.l2_loss(var))\n",
    "        return var\n",
    "    \n",
    "    def _bias_variable(self, shape, regularization=True):\n",
    "        \"\"\"Initializer for the bias\"\"\"\n",
    "        \n",
    "        initial = tf.constant_initializer(0.1)\n",
    "        var = tf.get_variable('bias', shape, tf.float32, initializer=initial)\n",
    "        if regularization:\n",
    "            self.regularizers.append(tf.nn.l2_loss(var))\n",
    "        return var\n",
    "    \n",
    "\n",
    "    def frobenius_norm(self, tensor): \n",
    "        \"\"\"Computes the frobenius norm for a given tensor\"\"\"\n",
    "        \n",
    "        square_tensor = tf.square(tensor)\n",
    "        tensor_sum = tf.reduce_sum(square_tensor)\n",
    "        frobenius_norm = tf.sqrt(tensor_sum)\n",
    "        return frobenius_norm\n",
    "    \n",
    "    \n",
    "    def count_no_weights(self):\n",
    "        total_parameters = 0\n",
    "        for variable in tf.trainable_variables():\n",
    "            # shape is an array of tf.Dimension\n",
    "            shape = variable.get_shape()\n",
    "            variable_parameters = 1\n",
    "            for dim in shape:\n",
    "                variable_parameters *= dim.value\n",
    "            total_parameters += variable_parameters\n",
    "        print('#weights in the model: %d' % (total_parameters,))\n",
    "\n",
    "    def monetConv(self,x,U,Fout,K):\n",
    "        N, M, Fin = x.get_shape()  # N is the number of images\n",
    "                                   # M the number of vertices in the images\n",
    "                                   # Fin the number of features\n",
    "        N, M, Fin = int(N), int(M), int(Fin)\n",
    "        gaussian_kernels =[]\n",
    "        for j in range(K):\n",
    "            W = self._weight_variable([Fin, Fout],substr=\"weight{}\".format(j), regularization=False)\n",
    "            mu = self._weight_variable([1],substr=\"mu{}\".format(j), regularization=False)\n",
    "            sigma = self._weight_variable([1],substr=\"sigma{}\".format(j), regularization=False)\n",
    "            d_u = tf.sparse.to_dense(U)\n",
    "            zero = tf.constant(0, dtype=tf.float32)\n",
    "            bools = tf.not_equal(d_u, zero)\n",
    "            mask = tf.cast(bools,dtype=tf.float32)\n",
    "            gaussian= tf.exp((-.5*tf.square((d_u-mu))/ (tf.square(sigma))))*mask\n",
    "            #N, M, D -> M,N,D\n",
    "            x_ = tf.transpose(x,[1,0,2])\n",
    "            \n",
    "            #M,N,D->\n",
    "            x_r = tf.reshape(x,[x_.shape[0],-1])\n",
    "            patch = tf.matmul(gaussian,x_r)\n",
    "            patch = tf.reshape(patch,[M*N,Fin])\n",
    "            kernel = tf.matmul(patch,W)\n",
    "            kernel = tf.reshape(kernel,[M,N,Fout])\n",
    "            gaussian_kernels.append(kernel)\n",
    "\n",
    "        new_x = tf.add_n(gaussian_kernels)\n",
    "        new_x = tf.transpose(new_x,[1,0,2])\n",
    "        return new_x\n",
    "    \n",
    "    def monetConv(self,x,U,Fout,K):\n",
    "        N, M, Fin = x.get_shape()  # N is the number of images\n",
    "                                   # M the number of vertices in the images\n",
    "                                   # Fin the number of features\n",
    "        N, M, Fin = int(N), int(M), int(Fin)\n",
    "    \n",
    "    \n",
    "        j = M\n",
    "        W = self._weight_variable([Fin, Fout],substr=\"weight{}\".format(j), regularization=False)\n",
    "        mu = self._weight_variable([1,K],substr=\"mu{}\".format(j), regularization=False)\n",
    "        sigma = self._weight_variable([1,K],substr=\"sigma{}\".format(j), regularization=False)\n",
    "        d_u = tf.sparse.to_dense(U)\n",
    "        d_u = tf.expand_dims(d_u,[2])\n",
    "        zero = tf.constant(0, dtype=tf.float32)\n",
    "        bools = tf.not_equal(d_u, zero)\n",
    "        \n",
    "        mask = tf.cast(bools,dtype=tf.float32)\n",
    "        \n",
    "\n",
    "        #M,M,1\n",
    "        gaussian= tf.exp((-.5*tf.square((d_u-mu))/ (1e-14 + tf.square(sigma))))*mask\n",
    "        #N, M, D -> M,N,D\n",
    "        x_ = tf.transpose(x,[1,0,2])\n",
    "        #M,N,D->\n",
    "        x_r = tf.reshape(x,[x_.shape[0],-1])\n",
    "        #M,N*D\n",
    "        gaussian = tf.transpose(gaussian,[2,0,1])\n",
    "        gaussian = tf.reshape(gaussian,[-1,M])\n",
    "        patch = tf.matmul(gaussian,x_r)\n",
    "        patch = tf.reshape(patch,[K*M*N,Fin])\n",
    "        \n",
    "        kernel = tf.matmul(patch,W)\n",
    "        kernel = tf.reshape(kernel,[K,M,batch_size,Fout])\n",
    "\n",
    "        new_x = tf.reduce_sum(kernel,0)\n",
    "        new_x = tf.transpose(new_x,[1,0,2])\n",
    "        return new_x\n",
    "        \n",
    "    #Modules used by the graph convolutional network\n",
    "    def chebyshevConv(self, x, L, Fout, K): \n",
    "        \"\"\"Applies chebyshev polynomials over the graph (i.e. it makes a spectral convolution)\"\"\"\n",
    "        \n",
    "        N, M, Fin = x.get_shape()  # N is the number of images\n",
    "                                   # M the number of vertices in the images\n",
    "                                   # Fin the number of features\n",
    "        N, M, Fin = int(N), int(M), int(Fin)\n",
    "        \n",
    "        # Transform to Chebyshev basis. Here we apply the chebyshev polynomials. For simplicity instead of operating\n",
    "        # over the eigenvalues of the laplacians we apply directly the chebysev polynomials to the laplacians.\n",
    "        # This makes everything more efficient if the laplacians are actually represented as sparse matrices.\n",
    "        x0 = x # L^0 * x = I * x\n",
    "        x = tf.expand_dims(x0, 0)  # shape = 1 x N x M x Fin\n",
    "        def concat(x, x_):\n",
    "            x_ = tf.expand_dims(x_, 0)  # shape = 1 x M x Fin*N\n",
    "            return tf.concat([x, x_], 0)  # shape = 1 x N x M x Fin\n",
    "        if K > 1:\n",
    "            x0 = tf.transpose(x0, [1,2,0])\n",
    "            x0 = tf.reshape(x0, [M, Fin*N])\n",
    "            x1 = tf.sparse_tensor_dense_matmul(L, x0) #shape = M x Fin*N <- L^1 * x\n",
    "            x1 = tf.reshape(x1, [M, Fin, N]) # shape = M x Fin x N <- L^1 * x\n",
    "            x1 = tf.transpose(x1, [2,0,1]) # shape = N x M x Fin\n",
    "            x0 = tf.reshape(x0, [M, Fin, N])\n",
    "            x0 = tf.transpose(x0, [2,0,1]) # shape = N x M x Fin\n",
    "            x = concat(x, x1) # shape = 2 x N x M x Fin\n",
    "        for k in range(2, K):\n",
    "            x1 = tf.transpose(x1, [1,2,0])\n",
    "            x1 = tf.reshape(x1, [M, Fin*N])\n",
    "            x2 = tf.sparse_tensor_dense_matmul(L, x1) # shape = M x Fin*N <- L^k * x\n",
    "            x2 = tf.reshape(x2, [M, Fin, N]) # shape = M x Fin x N <- L^1 * x\n",
    "            x2 = tf.transpose(x2, [2,0,1])\n",
    "            x1 = tf.reshape(x1, [M, Fin, N])\n",
    "            x1 = tf.transpose(x1, [2,0,1]) # shape = N x M x Fin\n",
    "            x2 = 2 * x2 - x0  # shape = N x M x Fin <- recursive definition of chebyshev polynomials\n",
    "            x = concat(x, x2) # shape = K x N x M x Fin\n",
    "            x0, x1 = x1, x2\n",
    "        x = tf.transpose(x, [1,2,3,0]) # shape = N x M x Fin x K\n",
    "        x = tf.reshape(x, [N*M, Fin*K])  # shape = N*M x Fin*K\n",
    "        \n",
    "        # Filter: Fout filters of order K applied over all the Fin features\n",
    "        W = self._weight_variable([Fin*K, Fout], regularization=False)\n",
    "        x = tf.matmul(x, W)  # N*M x Fout\n",
    "        return tf.reshape(x, [N, M, Fout])  # N x M x Fout\n",
    "\n",
    "    def b1relu(self, x):\n",
    "        \"\"\"Applies bias and ReLU. One bias per filter.\"\"\"\n",
    "        N, M, F = x.get_shape()\n",
    "        b = self._bias_variable([1, 1, int(F)], regularization=False)\n",
    "        return tf.nn.relu(x + b) #add the bias to the convolutive layer\n",
    "\n",
    "\n",
    "    def mpool1(self, x, p):\n",
    "        \"\"\"Max pooling of size p. Should be a power of 2 (this is possible thanks to the reordering we previously did).\"\"\"\n",
    "        if p > 1:\n",
    "            x = tf.expand_dims(x, 3)  # shape = N x M x F x 1\n",
    "            x = tf.nn.max_pool(x, ksize=[1,p,1,1], strides=[1,p,1,1], padding='SAME')\n",
    "            return tf.squeeze(x, [3])  # shape = N x M/p x F\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def fc(self, x, Mout, relu=True):\n",
    "        \"\"\"Fully connected layer with Mout features.\"\"\"\n",
    "        N, Min = x.get_shape()\n",
    "        W = self._weight_variable([int(Min), Mout],\"\", regularization=True)\n",
    "        b = self._bias_variable([Mout], regularization=True)\n",
    "        x = tf.matmul(x, W) + b\n",
    "        return tf.nn.relu(x) if relu else x\n",
    "    \n",
    "    #function used for extracting the result of our model\n",
    "    def _inference(self, x, dropout): #definition of the model\n",
    "        \n",
    "        # Graph convolutional layers.\n",
    "        x = tf.expand_dims(x, 2)  # N x M x F=1\n",
    "        for i in range(len(self.p)):\n",
    "            with tf.variable_scope('cgconv{}'.format(i+1)):\n",
    "                with tf.name_scope('filter'):\n",
    "                    x = self.monetConv(x,self.L[i*2],self.F[i],self.K[i])\n",
    "#                     x = self.chebyshevConv(x, self.L[i*2], self.F[i], self.K[i])\n",
    "                with tf.name_scope('bias_relu'):\n",
    "                    x = self.b1relu(x)\n",
    "                with tf.name_scope('pooling'):\n",
    "                    x = self.mpool1(x, self.p[i])\n",
    "         \n",
    "        # Fully connected hidden layers.\n",
    "        N, M, F = x.get_shape()\n",
    "        x = tf.reshape(x, [int(N), int(M*F)])  # N x M\n",
    "        for i,M in enumerate(self.M[:-1]): #apply a fully connected layer for each layer defined in M\n",
    "                                           #(we discard the last value in M since it contains the number of classes we have\n",
    "                                           #to predict)\n",
    "            with tf.variable_scope('fc{}'.format(i+1)):\n",
    "                x = self.fc(x, M)\n",
    "                x = tf.nn.dropout(x, dropout)\n",
    "        \n",
    "        # Logits linear layer, i.e. softmax without normalization.\n",
    "        with tf.variable_scope('logits'):\n",
    "            x = self.fc(x, self.M[-1], relu=False)\n",
    "        return x\n",
    "    \n",
    "    def convert_coo_to_sparse_tensor(self, L):\n",
    "        indices = np.column_stack((L.row, L.col))\n",
    "        L = tf.SparseTensor(indices, L.data.astype('float32'), L.shape)\n",
    "        L = tf.sparse_reorder(L)\n",
    "        return L\n",
    "        \n",
    "    \n",
    "    def __init__(self, p, K, F, M, M_0, batch_size, L,\n",
    "                 decay_steps, decay_rate, learning_rate=1e-4, momentum=0.9, regularization=5e-4, \n",
    "                 idx_gpu = '/gpu:0'):\n",
    "        self.regularizers = list() #list of regularization l2 loss for multiple variables\n",
    "        \n",
    "        self.p = p #dimensions of the pooling layers\n",
    "        self.K = K #List of polynomial orders, i.e. filter sizes or number of hops\n",
    "        self.F = F #Number of features of convolutional layers\n",
    "        \n",
    "        self.M = M #Number of neurons in fully connected layers\n",
    "        \n",
    "        self.M_0 = M_0 #number of elements in the first graph \n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        #definition of some learning parameters\n",
    "        self.decay_steps = decay_steps\n",
    "        self.decay_rate = decay_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        \n",
    "        with tf.Graph().as_default() as g:\n",
    "                self.graph = g\n",
    "                tf.set_random_seed(0)\n",
    "                with tf.device(idx_gpu):\n",
    "                        #definition of placeholders\n",
    "                        self.L = [self.convert_coo_to_sparse_tensor(c_L.tocoo()) for c_L in L]\n",
    "                        self.ph_data = tf.placeholder(tf.float32, (self.batch_size, M_0), 'data')\n",
    "                        self.ph_labels = tf.placeholder(tf.int32, (self.batch_size), 'labels')\n",
    "                        self.ph_dropout = tf.placeholder(tf.float32, (), 'dropout')\n",
    "                    \n",
    "                        #Model construction\n",
    "                        self.logits = self._inference(self.ph_data, self.ph_dropout)\n",
    "                        \n",
    "                        #Definition of the loss function\n",
    "                        with tf.name_scope('loss'):\n",
    "                            self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, \n",
    "                                                                                                labels=self.ph_labels)\n",
    "                            self.cross_entropy = tf.reduce_mean(self.cross_entropy)\n",
    "                        with tf.name_scope('regularization'):\n",
    "                            self.regularization *= tf.add_n(self.regularizers)\n",
    "                        self.loss = self.cross_entropy + self.regularization\n",
    "                        \n",
    "                        #Solver Definition\n",
    "                        with tf.name_scope('training'):\n",
    "                            # Learning rate.\n",
    "                            global_step = tf.Variable(0, name='global_step', trainable=False) #used for counting how many iterations we have done\n",
    "                            if decay_rate != 1: #applies an exponential decay of the lr wrt the number of iterations done\n",
    "                                learning_rate = tf.train.exponential_decay(\n",
    "                                        learning_rate, global_step, decay_steps, decay_rate, staircase=True)\n",
    "                            # Optimizer.\n",
    "                            if momentum == 0:\n",
    "                                optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "                            else: #applies momentum for increasing the robustness of the gradient \n",
    "                                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "                            grads = optimizer.compute_gradients(self.loss)\n",
    "                            self.op_gradients = optimizer.apply_gradients(grads, global_step=global_step) \n",
    "                            \n",
    "                        #Computation of the norm gradients (useful for debugging)\n",
    "                        self.var_grad = tf.gradients(self.loss, tf.trainable_variables())\n",
    "                        self.norm_grad = self.frobenius_norm(tf.concat([tf.reshape(g, [-1]) for g in self.var_grad], 0))\n",
    "\n",
    "                        #Extraction of the predictions and computation of accuracy\n",
    "                        self.predictions = tf.cast(tf.argmax(self.logits, dimension=1), tf.int32)\n",
    "                        self.accuracy = 100 * tf.contrib.metrics.accuracy(self.predictions, self.ph_labels)\n",
    "        \n",
    "                        # Create a session for running Ops on the Graph.\n",
    "                        config = tf.ConfigProto(allow_soft_placement = True)\n",
    "                        config.gpu_options.allow_growth = True\n",
    "                        self.session = tf.Session(config=config)\n",
    "\n",
    "                        # Run the Op to initialize the variables.\n",
    "                        init = tf.global_variables_initializer()\n",
    "                        self.session.run(init)\n",
    "                        \n",
    "                        self.count_no_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolutional parameters\n",
    "p = [4, 4]   #Dimensions of the pooling layers\n",
    "K = [25, 25] #List of polynomial orders, i.e. filter sizes or number of hops\n",
    "F = [32, 64] #Number of features of convolutional layers\n",
    "\n",
    "#FC parameters\n",
    "C = max(mnist.train.labels) + 1 #Number of classes we have\n",
    "M = [512, C] #Number of neurons in fully connected layers\n",
    "\n",
    "#Solver parameters\n",
    "batch_size = 100\n",
    "decay_steps = mnist.train.num_examples / batch_size #number of steps to do before decreasing the learning rate\n",
    "decay_rate = 0.95 #how much decreasing the learning rate\n",
    "learning_rate = 0.02\n",
    "# learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "regularization = 5e-4\n",
    "\n",
    "#Definition of keep probabilities for dropout layers\n",
    "dropout_training = 0.5\n",
    "dropout_val_test = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/heng/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-8-553f9c4ece42>:206: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-8-553f9c4ece42>:283: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "#weights in the model: 2056686\n"
     ]
    }
   ],
   "source": [
    "#Construction of the learning obj\n",
    "M_0 = A2[0].shape[0] #number of elements in the first graph\n",
    "learning_obj = ChebNet(p, K, F, M, M_0, batch_size, A2,\n",
    "                       decay_steps, decay_rate,\n",
    "                       learning_rate=learning_rate, regularization=regularization,\n",
    "                       momentum=momentum)\n",
    "\n",
    "#definition of overall number of training iterations and validation frequency\n",
    "num_iter_val = 100\n",
    "num_total_iter_training = 200000\n",
    "\n",
    "num_iter = 0\n",
    "\n",
    "list_training_loss = list()\n",
    "list_training_norm_grad = list()\n",
    "list_val_accuracy = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Sum:0\", shape=(1024, 2, 32), dtype=float32)\n",
      "Tensor(\"transpose_2:0\", shape=(2, 1024, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def convert_coo_to_sparse_tensor( L):\n",
    "    indices = np.column_stack((L.row, L.col))\n",
    "    L = tf.SparseTensor(indices, L.data.astype('float32'), L.shape)\n",
    "    L = tf.sparse_reorder(L)\n",
    "    return L\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "init =tf.global_variables_initializer()\n",
    "F = [32, 64]\n",
    "with tf.Session() as sess:\n",
    "    K=J=25\n",
    "    F_in = 1\n",
    "    F_out = F[0]\n",
    "    batch_size =2\n",
    "    M= A2[0].shape[0]\n",
    "    x = tf.ones((batch_size,M,1))*2\n",
    "    u = convert_coo_to_sparse_tensor(A2[0].tocoo())\n",
    "\n",
    "\n",
    "    j=1\n",
    "    w = tf.get_variable('w{}'.format(j),shape=(F_in,F_out), dtype=tf.float32)\n",
    "    mu = tf.get_variable('mu{}'.format(j),shape=(1,J), dtype=tf.float32)\n",
    "    sigma = tf.get_variable('sigma{}'.format(j),shape=(1,J), dtype=tf.float32)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    u = convert_coo_to_sparse_tensor(A2[0].tocoo())\n",
    "#         print(u.values.shape)\n",
    "#         s_mu = tf.broadcast_to(mu,u.values.shape)\n",
    "#         s_sigma = tf.broadcast_to(sigma,u.values.shape)\n",
    "\n",
    "#         a = u*u\n",
    "    d_u = tf.sparse.to_dense(u)\n",
    "    d_u = tf.expand_dims(d_u,[2])\n",
    "    zero = tf.constant(0, dtype=tf.float32)\n",
    "    bools = tf.not_equal(d_u, zero)\n",
    "    mask = tf.cast(bools,dtype=tf.float32)\n",
    "\n",
    "    gaussian= tf.exp((-.5*tf.square((d_u-mu))/ (1e-14 + tf.square(sigma))))*mask\n",
    "#         vals_ = tf.boolean_mask(gaussian,mask=bools)\n",
    "#         sum_gaussian = tf.reduce_sum(gaussian,axis=1)\n",
    "#         print(patch.shape)\n",
    "#         s_gaussian = tf.sparse.SparseTensor(values=vals_,indices=u.indices,dense_shape=gaussian.shape)\n",
    "#         temp = tf.sparse.matmul(s_gaussian,x_)\n",
    "    # Nodes,batch_size,n_dim\n",
    "    x_ = tf.transpose(x,[1,0,2])\n",
    "    #remove F_in\n",
    "    x_r = tf.reshape(x,[x_.shape[0],-1])\n",
    "    gaussian = tf.transpose(gaussian,[2,0,1])\n",
    "    gaussian = tf.reshape(gaussian,[-1,M])\n",
    "    patch = tf.matmul(gaussian,x_r)\n",
    "    \n",
    "    patch = tf.reshape(patch,[K*M*batch_size,F_in])\n",
    "    \n",
    "# #         tf.reshape()\n",
    "# #         tf.reshape(patch,[])\n",
    "# #         gaussian_kernels.append(patch)\n",
    "    kernel = tf.matmul(patch,w)\n",
    "    kernel = tf.reshape(kernel,[K,M,batch_size,F_out])\n",
    "    new_x = tf.reduce_sum(kernel,0)\n",
    "    print(new_x)\n",
    "#     kernel = tf.reshape(kernel,[M,batch_size,F_out])\n",
    "#     gaussian_kernels.append(kernel)\n",
    "        \n",
    "# #     new_x = tf.add_n(gaussian_kernels)\n",
    "# #     print(new_x.shape)\n",
    "    new_x = tf.transpose(new_x,[1,0,2])\n",
    "    print(new_x)\n",
    "#     print(new_x.shape)\n",
    "#     tf.reshape([])\n",
    "# #     print(gaussian_kernels[0])\n",
    "#     print(new_x.eval().shape)\n",
    "#         print(patch.shape,gaussian.shape,x_r.shape\n",
    "#         tf.reduce\n",
    "#         x_ = tf.transpose(x,[1,2])\n",
    "#         temp = tf.matmul(gaussian,x)\n",
    "#         gaussian_kernels.append(gaussian)\n",
    "#     tf.reduce_sum(gaussian,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/heng/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "(2, 1024, 32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "init =tf.global_variables_initializer()\n",
    "F = [32, 64]\n",
    "with tf.Session() as sess:\n",
    "    J=2\n",
    "    F_in = 1\n",
    "    F_out = F[0]\n",
    "    batch_size =2\n",
    "    M= A2[0].shape[0]\n",
    "    x = tf.ones((batch_size,M,1))*2\n",
    "    u = convert_coo_to_sparse_tensor(A2[0].tocoo())\n",
    "\n",
    "    gaussian_kernels =[]\n",
    "    for j in range(J):\n",
    "        w = tf.get_variable('w{}'.format(j),shape=(F_in,F_out), dtype=tf.float32)\n",
    "        mu = tf.get_variable('mu{}'.format(j),shape=(1), dtype=tf.float32)\n",
    "        sigma = tf.get_variable('sigma{}'.format(j),shape=(1), dtype=tf.float32)\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        \n",
    "        u = convert_coo_to_sparse_tensor(A2[0].tocoo())\n",
    "#         print(u.values.shape)\n",
    "#         s_mu = tf.broadcast_to(mu,u.values.shape)\n",
    "#         s_sigma = tf.broadcast_to(sigma,u.values.shape)\n",
    "\n",
    "#         a = u*u\n",
    "        d_u = tf.sparse.to_dense(u)\n",
    "        zero = tf.constant(0, dtype=tf.float32)\n",
    "        bools = tf.not_equal(d_u, zero)\n",
    "        mask = tf.cast(bools,dtype=tf.float32)\n",
    "        \n",
    "        gaussian= tf.exp((-.5*tf.square((d_u-mu))/ (1e-14 + tf.square(sigma))))*mask\n",
    "#         vals_ = tf.boolean_mask(gaussian,mask=bools)\n",
    "#         sum_gaussian = tf.reduce_sum(gaussian,axis=1)\n",
    "#         print(patch.shape)\n",
    "#         s_gaussian = tf.sparse.SparseTensor(values=vals_,indices=u.indices,dense_shape=gaussian.shape)\n",
    "#         temp = tf.sparse.matmul(s_gaussian,x_)\n",
    "        # Nodes,batch_size,n_dim\n",
    "        x_ = tf.transpose(x,[1,0,2])\n",
    "        #remove F_in\n",
    "        x_r = tf.reshape(x,[x_.shape[0],-1])\n",
    "        patch = tf.matmul(gaussian,x_r)\n",
    "        patch = tf.reshape(patch,[M*batch_size,F_in])\n",
    "    \n",
    "#         tf.reshape()\n",
    "#         tf.reshape(patch,[])\n",
    "#         gaussian_kernels.append(patch)\n",
    "        kernel = tf.matmul(patch,w)\n",
    "        kernel = tf.reshape(kernel,[M,batch_size,F_out])\n",
    "        gaussian_kernels.append(kernel)\n",
    "        \n",
    "    new_x = tf.add_n(gaussian_kernels)\n",
    "#     print(new_x.shape)\n",
    "    new_x = tf.transpose(new_x,[1,0,2])\n",
    "    print(new_x.shape)\n",
    "#     tf.reshape([])\n",
    "# #     print(gaussian_kernels[0])\n",
    "#     print(new_x.eval().shape)\n",
    "#         print(patch.shape,gaussian.shape,x_r.shape\n",
    "#         tf.reduce\n",
    "#         x_ = tf.transpose(x,[1,2])\n",
    "#         temp = tf.matmul(gaussian,x)\n",
    "#         gaussian_kernels.append(gaussian)\n",
    "#     tf.reduce_sum(gaussian,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_coo_to_sparse_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bc3f6aed9c7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mM\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mA2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_coo_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mgaussian_kernels\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_coo_to_sparse_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "init =tf.global_variables_initializer()\n",
    "F = [32, 64]\n",
    "with tf.Session() as sess:\n",
    "    J=2\n",
    "    F_in = 32\n",
    "    F_out = F[1]\n",
    "    batch_size =2\n",
    "    M= A2[2].shape[0]\n",
    "    x = tf.ones((batch_size,M,F_in))*2\n",
    "    u = convert_coo_to_sparse_tensor(A2[0].tocoo())\n",
    "\n",
    "    gaussian_kernels =[]\n",
    "    for j in range(J):\n",
    "        w = tf.get_variable('w{}'.format(j),shape=(F_in,F_out), dtype=tf.float32)\n",
    "        mu = tf.get_variable('mu{}'.format(j),shape=(1), dtype=tf.float32)\n",
    "        sigma = tf.get_variable('sigma{}'.format(j),shape=(1), dtype=tf.float32)\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        \n",
    "        u = convert_coo_to_sparse_tensor(A2[2].tocoo())\n",
    "#         print(u.values.shape)\n",
    "#         s_mu = tf.broadcast_to(mu,u.values.shape)\n",
    "#         s_sigma = tf.broadcast_to(sigma,u.values.shape)\n",
    "\n",
    "#         a = u*u\n",
    "        d_u = tf.sparse.to_dense(u)\n",
    "        zero = tf.constant(0, dtype=tf.float32)\n",
    "        bools = tf.not_equal(d_u, zero)\n",
    "        mask = tf.cast(bools,dtype=tf.float32)\n",
    "        \n",
    "        gaussian= tf.exp((-.5*tf.square((d_u-mu))/ (1e-14 + tf.square(sigma))))*mask\n",
    "#         vals_ = tf.boolean_mask(gaussian,mask=bools)\n",
    "#         sum_gaussian = tf.reduce_sum(gaussian,axis=1)\n",
    "#         print(patch.shape)\n",
    "#         s_gaussian = tf.sparse.SparseTensor(values=vals_,indices=u.indices,dense_shape=gaussian.shape)\n",
    "#         temp = tf.sparse.matmul(s_gaussian,x_)\n",
    "        # Nodes,batch_size,n_dim\n",
    "        \n",
    "        x_ = tf.transpose(x,[1,0,2])\n",
    "        #remove F_in\n",
    "        x_r = tf.reshape(x,[x_.shape[0],-1])\n",
    "\n",
    "        patch = tf.matmul(gaussian,x_r)\n",
    "        \n",
    "        patch = tf.reshape(patch,[M*batch_size,F_in])\n",
    "    \n",
    "\n",
    "# #         gaussian_kernels.append(patch)\n",
    "        kernel = tf.matmul(patch,w)\n",
    "        kernel = tf.reshape(kernel,[M,batch_size,F_out])\n",
    "        print('gaussian {},x_{},x_r{},patch{},kernel_{}'.format(gaussian.shape,x_.shape,x_r.shape,patch.shape,kernel.shape))\n",
    "        gaussian_kernels.append(kernel)\n",
    "        \n",
    "#     new_x = tf.add_n(gaussian_kernels)\n",
    "# #     print(new_x.shape)\n",
    "#     new_x = tf.transpose(new_x,[1,0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRN] iter = 000, cost = 3.27e+01, |grad| = 1.32e+03 (4.30e+00s)\n",
      "[VAL] iter = 000, acc = 8.68 (1.38e+01s)\n",
      "[TRN] iter = 100, cost = 1.36e+04, |grad| = 3.70e+00 (1.02e+00s)\n",
      "[VAL] iter = 100, acc = 11.26 (1.44e+01s)\n",
      "[TRN] iter = 200, cost = 1.33e+04, |grad| = 3.66e+00 (1.18e+00s)\n",
      "[VAL] iter = 200, acc = 11.26 (1.48e+01s)\n",
      "[TRN] iter = 300, cost = 1.31e+04, |grad| = 3.62e+00 (1.03e+00s)\n",
      "[VAL] iter = 300, acc = 9.86 (1.45e+01s)\n",
      "[TRN] iter = 400, cost = 1.28e+04, |grad| = 3.58e+00 (1.07e+00s)\n",
      "[VAL] iter = 400, acc = 11.26 (1.47e+01s)\n",
      "[TRN] iter = 500, cost = 1.26e+04, |grad| = 3.55e+00 (1.05e+00s)\n",
      "[VAL] iter = 500, acc = 11.26 (1.44e+01s)\n",
      "[TRN] iter = 600, cost = 1.23e+04, |grad| = 3.51e+00 (1.05e+00s)\n",
      "[VAL] iter = 600, acc = 11.26 (1.44e+01s)\n",
      "[TRN] iter = 700, cost = 1.21e+04, |grad| = 3.48e+00 (1.02e+00s)\n",
      "[VAL] iter = 700, acc = 11.26 (1.44e+01s)\n",
      "[TRN] iter = 800, cost = 1.19e+04, |grad| = 3.45e+00 (1.58e+00s)\n",
      "[VAL] iter = 800, acc = 11.26 (1.68e+01s)\n",
      "[TRN] iter = 900, cost = 1.16e+04, |grad| = 3.41e+00 (1.05e+00s)\n",
      "[VAL] iter = 900, acc = 11.26 (1.49e+01s)\n",
      "[TRN] iter = 1000, cost = 1.14e+04, |grad| = 3.38e+00 (1.02e+00s)\n",
      "[VAL] iter = 1000, acc = 11.26 (1.44e+01s)\n",
      "[TRN] iter = 1100, cost = 1.12e+04, |grad| = 3.35e+00 (1.05e+00s)\n",
      "[VAL] iter = 1100, acc = 11.26 (1.44e+01s)\n",
      "[TRN] iter = 1200, cost = 1.10e+04, |grad| = 3.32e+00 (1.72e+00s)\n",
      "[VAL] iter = 1200, acc = 11.26 (1.57e+01s)\n",
      "[TRN] iter = 1300, cost = 1.08e+04, |grad| = 3.29e+00 (1.07e+00s)\n",
      "[VAL] iter = 1300, acc = 11.26 (1.46e+01s)\n",
      "[TRN] iter = 1400, cost = 1.06e+04, |grad| = 3.26e+00 (1.05e+00s)\n",
      "[VAL] iter = 1400, acc = 11.26 (1.47e+01s)\n",
      "[TRN] iter = 1500, cost = 1.04e+04, |grad| = 3.23e+00 (1.23e+00s)\n",
      "[VAL] iter = 1500, acc = 11.00 (1.99e+01s)\n"
     ]
    }
   ],
   "source": [
    "#training and validation\n",
    "indices = collections.deque() #queue that will contain a permutation of the training indexes\n",
    "for k in range(num_iter, num_total_iter_training):\n",
    "    \n",
    "    #Construction of the training batch\n",
    "    if len(indices) < batch_size: # Be sure to have used all the samples before using one a second time.\n",
    "        indices.extend(np.random.permutation(train_data.shape[0])) #reinitialize the queue of indices\n",
    "    idx = [indices.popleft() for i in range(batch_size)] #extract the current batch of samples\n",
    "    \n",
    "    #data extraction\n",
    "    batch_data, batch_labels = train_data[idx,:], train_labels[idx] \n",
    "    \n",
    "    feed_dict = {learning_obj.ph_data: batch_data, \n",
    "                 learning_obj.ph_labels: batch_labels, \n",
    "                 learning_obj.ph_dropout: dropout_training}\n",
    "    \n",
    "    #Training\n",
    "    tic = time.time()\n",
    "    _, current_training_loss, norm_grad = learning_obj.session.run([learning_obj.op_gradients, \n",
    "                                                                    learning_obj.loss, \n",
    "                                                                    learning_obj.norm_grad], feed_dict = feed_dict) \n",
    "    training_time = time.time() - tic\n",
    "    \n",
    "    list_training_loss.append(current_training_loss)\n",
    "    list_training_norm_grad.append(norm_grad)\n",
    "    \n",
    "    if (np.mod(num_iter, num_iter_val)==0): #validation\n",
    "        msg = \"[TRN] iter = %03i, cost = %3.2e, |grad| = %.2e (%3.2es)\" \\\n",
    "                    % (num_iter, list_training_loss[-1], list_training_norm_grad[-1], training_time)\n",
    "        print(msg)\n",
    "        \n",
    "        #Validation Code\n",
    "        tic = time.time()\n",
    "        val_accuracy = 0\n",
    "        for begin in range(0, val_data.shape[0], batch_size):\n",
    "            end = begin + batch_size\n",
    "            end = min([end, val_data.shape[0]])\n",
    "            \n",
    "            #data extraction\n",
    "            batch_data = np.zeros((end-begin, val_data.shape[1]))\n",
    "            batch_data = val_data[begin:end,:]\n",
    "            batch_labels = np.zeros(batch_size)\n",
    "            batch_labels[:end-begin] = val_labels[begin:end]\n",
    "            \n",
    "            feed_dict = {learning_obj.ph_data: batch_data, \n",
    "                         learning_obj.ph_labels: batch_labels,\n",
    "                         learning_obj.ph_dropout: dropout_val_test}\n",
    "            \n",
    "            batch_accuracy = learning_obj.session.run(learning_obj.accuracy, feed_dict)\n",
    "            val_accuracy += batch_accuracy*batch_data.shape[0]\n",
    "        val_accuracy = val_accuracy/val_data.shape[0]\n",
    "        val_time = time.time() - tic\n",
    "        msg = \"[VAL] iter = %03i, acc = %4.2f (%3.2es)\" % (num_iter, val_accuracy, val_time)\n",
    "        print(msg)\n",
    "    num_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TST] iter = 21000, acc = 98.91 (2.71e+00s)\n"
     ]
    }
   ],
   "source": [
    "#Test code\n",
    "tic = time.time()\n",
    "test_accuracy = 0\n",
    "for begin in range(0, test_data.shape[0], batch_size):\n",
    "    end = begin + batch_size\n",
    "    end = min([end, test_data.shape[0]])\n",
    "            \n",
    "    batch_data = np.zeros((end-begin, test_data.shape[1]))\n",
    "    batch_data = test_data[begin:end,:]\n",
    "            \n",
    "    feed_dict = {learning_obj.ph_data: batch_data, learning_obj.ph_dropout: 1}\n",
    "            \n",
    "    batch_labels = np.zeros(batch_size)\n",
    "    batch_labels[:end-begin] = test_labels[begin:end]\n",
    "    feed_dict[learning_obj.ph_labels] = batch_labels\n",
    "            \n",
    "    batch_accuracy = learning_obj.session.run(learning_obj.accuracy, feed_dict)\n",
    "    test_accuracy += batch_accuracy*batch_data.shape[0]\n",
    "test_accuracy = test_accuracy/test_data.shape[0]\n",
    "test_time = time.time() - tic\n",
    "msg = \"[TST] iter = %03i, acc = %4.2f (%3.2es)\" % (num_iter, test_accuracy, test_time)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
